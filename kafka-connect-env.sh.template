#### Command to execute

KC_CMD=
## Example: /usr/bin/connect-standalone /etc/kafka/connect-standalone.properties /etc/kafka-connect-jdbc/jdbc-sink.properties


#### Heap setting for Connector
#KAFKA_HEAP_OPTS="-Xmx1536m -Xms1024m"

#### Kafka Connect Kafka Related Properties

# Kafka Endpoint

KAFKA_BOOTSTRAP_URL=

# Converter related params. Format for Schema Registry URL is http://host:port 

KEY_SR_URL=
VALUE_SR_URL=
KEY_CONVERTER_SCHEMAS_ENABLE=
VALUE_CONVERTER_SCHEMAS_ENABLE=
KEY_CONVERTER=
VALUE_CONVERTER=

# Offsets file

KC_OFFSETS_FILE=

## Absolute path within the Docker container. use host networking when possible, otherwise may not be reachable.
### Both the keystore and trustore are provided as base64 encoded strings below.
# SSL Properties

SSL_KEY_PW=
SSL_KEYSTORE_PW=
SSL_TRUSTSTORE_PW=
TRUSTSTORE=
KEYSTORE=
FULL_PATH_TO_KS=
FULL_PATH_TO_TS=

######## Consumer client tuning properties. Defaults will work in most cases. 
######## Please review the Apache Kafka documentation before changing any parameter below.

CONSUMER_REQUEST_TIMEOUT_MS=
CONSUMER_CLIENT_ID=
CONSUMER_KEY_SERIALIZER=
CONSUMER_VALUE_SERIALIZER=
CONSUMER_GROUP_ID=
CONSUMER_FETCH_MIN_BYTES=
CONSUMER_HEARTBEAT_INTERVAL_MS=
CONSUMER_MAX_PARTITION_FETCH_BYTES=
CONSUMER_SESSION_TIMEOUT_MS=
CONSUMER_AUTO_OFFSET_RESET=
CONSUMER_AUTO_OFFSET_RESET=
CONSUMER_FETCH_MAX_BYTES=
CONSUMER_ISOLATION_LEVEL=
CONSUMER_MAX_POLL_INTERVAL_MS=
CONSUMER_MAX_POLL_RECORDS=
CONSUMER_PARTITION_ASSIGNMENT_STRATEGY=
CONSUMER_RECEIVE_BUFFER_BYTES=
CONSUMER_FETCH_MAX_WAIT_MS=
CONSUMER_METADATA_MAX_AGE_MS=
CONSUMER_METRIC_REPORTERS=
CONSUMER_METRICS_NUM_SAMPLES=
CONSUMER_METRICS_RECORDING_LEVEL=
CONSUMER_RECONNECT_BACKOFF_MAX_MS=
CONSUMER_METRICS_SAMPLE_WINDOW_MS=
CONSUMER_RECEIVE_BUFFER_BYTES=
CONSUMER_SEND_BUFFER_BYTES=
CONSUMER_CONNECTIONS_MAX_IDLE_MS=
CONSUMER_RECONNECT_BACKOFF_MAX_MS=
CONSUMER_RECONNECT_BACKOFF_MS=
CONSUMER_RETRY_BACKOFF_MS=
CONSUMER_AUTO_COMMIT_INTERVAL_MS=

###### Producer client tuning properties. Defaults will work in most cases. 
###### Please review the Apache Kafka documentation before changing any parameter below.

PRODUCER_KEY_SERIALIZER=
PRODUCER_VALUE_SERIALIZER=
PRODUCER_ACKS=
PRODUCER_BUFFER_MEMORY=
PRODUCER_COMPRESSION_TYPE=
PRODUCER_RETRIES=
PRODUCER_BATCH_SIZE=
PRODUCER_CLIENT_ID=
PRODUCER_CONNECTIONS_MAX_IDLE_MS=
PRODUCER_LINGER_MS=
PRODUCER_MAX_BLOCK_MS=
PRODUCER_MAX_BLOCK_MS=
PRODUCER_PARTITIONER_CLASS=
PRODUCER_RECEIVE_BUFFER_BYTES=
PRODUCER_RECEIVE_TIMEOUT_MS=
PRODUCER_SEND_BUFFER_BYTES=
PRODUCER_SSL_ENABLED_PROTOCOLS=
PRODUCER_SSL_PROTOCOL=
PRODUCER_SSL_PROVIDER=
PRODUCER_SSL_TRUSTSTORE_TYPE=
PRODUCER_ENABLE_IDEMPOTENCE=
PRODUCER_MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=
PRODUCER_METADATA_MAX_AGE_MS=
PRODUCER_METRIC_REPORTERS=
PRODUCER_METRICS_NUM_SAMPLES=
PRODUCER_METRICS_RECORDING_LEVEL=
PRODUCER_METRICS_SAMPLE_WINDOW_MS=
PRODUCER_RECONNECT_BACKOFF_MAX_MS=
PRODUCER_RECONNECT_BACKOFF_MS=
PRODUCER_RETRY_BACKOFF_MS=
PRODUCER_TRANSACTION_TIMEOUT_MS=
PRODUCER_TRANSACTIONAL_ID=


#### JDBC source related properties

SOURCE_CONNECTOR_NAME=
SOURCE_JDBC_URL=
SOURCE_MODE=
SOURCE_QUERY=
SOURCE_TOPIC=
SOURCE_POLL_INTERVAL_MS=
SOURCE_TIMESTAMP_COLUMN_NAME=


#### JDBC Sink related properties

SINK_CONNECTOR_NAME=
SINK_TOPICS=
SINK_AUTO_CREATE=
SINK_AUTO_EVOLVE=
SINK_TABLE_NAME_FORMAT=

#### S3 Sink related properties

S3_SINK_NAME=
S3_SINK_TOPIC=
S3_SINK_FLUSH_SIZE=
S3_SINK_BUCKET_NAME=
S3_SINK_REGION=
S3_SINK_PART_SIZE=
S3_SINK_STORAGE_CLASS=
S3_SINK_STORE_URL=
S3_SINK_TOPICS_DIR=

### File Source related properties.

FILE_SOURCE_CONNECTOR_NAME=file-source-connector
FILE_SOURCE_CONNECTOR_CLASS=FileStreamSource
FILE_SOURCE_TASKS_MAX=1
FILE_SOURCE_PATH_TO_SOURCE_FILE=/path/to/local/file.txt
FILE_SOURCE_SOURCE_TOPIC=<topic-name>
